# -*- coding: utf-8 -*-
"""hierarchicalClustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pIQ3fQ2W9axmbJq4g3AI4ix_KxYJhvlZ

# (2) PRAKTIKUM HIERARCHICAL CLUSTERING

## NAMA:Bisma Ridho Pambudi

## NIM:10319002

***Catatan:***
1. Praktikum bersifat individual, namun berdiskusi secukupnya di dalam grup masing-masing
2. **Laporan final dalam format PDF** sebagai hasil konversi Notebook ke PDF dan disubmit di edunex. Bila ada slot informasi yang dapat dientry di edunex, tuliskan tautan ke Google Colab Notebook tersebut.

## Jenis-jenis Hierarchical Clustering (HC):
1. Agglomerative: pendekatan bottom-up. Diawali dengan anggapan setiap satu titik adalah satu cluster, lalu proses cluster merger berlanjut setelahnya hingga jumlah cluster menjadi satu.
2. Divisive: pendekatan top-bottom. Diawali dengan anggapan hanya ada satu cluster, lalu proses separatis berlanjut hingga jumlah cluster adalah sejumlah titik data.


### Algoritma Agglomerative Clustering:
1. Setiap satu titik data dianggap sebagai satu cluster
2. Ambil dua titik titik terdekat dan lakukan merger menjadi satu cluster
3. Ulangi langkah 2 hingga jumlah total cluster adalah 1

Beberapa contoh cara memilih titik-titik terdekat:
1. Ambil dua titik data dengan jarak terdekat (dan kedua terdekat)
2. Rerata jarak
3. Jarak centroid
4. Titik terjauh

Semua informasi jarak dan (hirarki) clustering di atas disimpan sebagai struktur data di dalam diagram bernama ***dendogram***. Pembatas (*threshold*) dapat dimanfaatkan dengan memperhatikan dendogram sehingga dapat diperoleh kesimpulan berapa jumlah cluster yang diperlukan.

### HC tidak direkomendasikan untuk dataset raksasa karena akan berat secara komputasional, harus menarget nilai hampiran dengan suku sisa berorde  $O(N^2 Log(N))$ (perhatikan faktor $N^2$)

### Perhatian:
Pada contoh kasus di bawah ini, akan dipelajari HC clustering pelanggan mall berdasarkan variabel ***penghasilan tahunan*** (indeks fitur ke-3) vs ***skor pengeluaran*** (indeks fitur ke-4). Di contoh persoalan ini, jarak yang sering berperan dalam clustering adalah ***jarak euclidean***.
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

df = pd.read_csv('Mall_Customers.csv')
print(df.head())

X = df.iloc[:, [3, 4]].values

# Using Dendrogram to find the optimal number of clusters
import scipy.cluster.hierarchy as sch
dendrogram = sch.dendrogram(sch.linkage(X, method='ward')) # The ward method tries to minimise the variance in each cluster
plt.title('Dendogram')
plt.xlabel('Pelanggan')
plt.ylabel('Jarak Euclidean')
plt.show()

"""#### Berdasarkan plot dendogram, berapa jumlah cluster yang disarankan?
*(Jawab dan tulis di sel ini: n_clusters = 3
  Berkaitan dengan pembatas / threshold apa dan sebesar berapa?
)*
"""

# Fitting hierarchical clustering model
from sklearn.cluster import AgglomerativeClustering
hc = AgglomerativeClustering(n_clusters=4, metric='euclidean', linkage='ward')
y_hc = hc.fit_predict(X)
y_hc

plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], color='red', s=60, label='Cluster 1', edgecolors='black')
plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], color='green', s=60, label='Cluster 2', edgecolors='black')
plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], color='blue',s=60, label='Cluster 3', edgecolors='black')
plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], color='yellow',s=60, label='Cluster 4', edgecolors='black')
#plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], color='pink',s=60, label='Cluster 5', edgecolors='black')
# cluster centres
#plt.scatter(hc.cluster_centers_[:, 0], hc.cluster_centers_[:, 1], color='magenta', s=100, label='Centroid',edgecolors='black')
plt.legend()
plt.title('Hierarchical Clustering')
plt.ylabel('Pendapatan Tahunan (ribu dollar)')
plt.xlabel('Skor Pengeluaran (1-100)')
plt.show()

"""### Tugas:

Gunakan HC Clustering untuk ide clustering lain seperti yang disinggung di bagian Tugas di modul 1 clustering. Kerjakan secara lengkap seperti contoh latihan di atas.
"""

df = pd.read_csv('/content/insurance.csv')
X = df.iloc[:, [2, 6]].values

# Cek tampilan beberapa baris atas dataframe
df.head()

# Using Dendrogram to find the optimal number of clusters
import scipy.cluster.hierarchy as sch
dendrogram = sch.dendrogram(sch.linkage(X, method='ward')) # The ward method tries to minimise the variance in each cluster
plt.title('Dendogram')
plt.xlabel('Subjek')
plt.ylabel('Jarak Euclidean')
plt.show()

"""Jumlah Cluster yang didapatkan adalah 3"""

# Fitting hierarchical clustering model
from sklearn.cluster import AgglomerativeClustering
hc = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')
y_hc = hc.fit_predict(X)
y_hc

plt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], color='red', s=60, label='Cluster 1', edgecolors='black')
plt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], color='green', s=60, label='Cluster 2', edgecolors='black')
plt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], color='blue',s=60, label='Cluster 3', edgecolors='black')
#plt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], color='yellow',s=60, label='Cluster 4', edgecolors='black')
#plt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], color='pink',s=60, label='Cluster 5', edgecolors='black')
# cluster centres
#plt.scatter(hc.cluster_centers_[:, 0], hc.cluster_centers_[:, 1], color='magenta', s=100, label='Centroid',edgecolors='black')
plt.legend()
plt.title('Hierarchical Clustering')
plt.ylabel('Charges')
plt.xlabel('BMI')
plt.show()